{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea09898c-42a7-4a0b-9903-2a1f312b0685",
   "metadata": {},
   "source": [
    "# Load Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ecc756d-0bce-4a1e-881b-11968de9da7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import random\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac4fcc2-d528-446c-95b7-edcc3024f29b",
   "metadata": {},
   "source": [
    "# Define the Dataset Path and Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e37dc099-9ff2-4e20-b995-1f2ba2f4b74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your dataset\n",
    "dataset_path = '/Users/lennox/Documents/machineLearning/data/tenAnimalsImages'\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff099675-230e-4939-a2dd-f2f0a99381a8",
   "metadata": {},
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0f37407-7a53-40e0-b808-f9dcd006009a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = ImageFolder(root=dataset_path, transform=transform)\n",
    "class_names = dataset.classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83845f39-5883-44d6-9ec1-a3581ad65f25",
   "metadata": {},
   "source": [
    "# Initialize DataLoader (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a65108a-05a8-472a-8cdb-073d8309bd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DataLoader (optional)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d149939c-c448-403c-b682-6447d331ff1e",
   "metadata": {},
   "source": [
    "# Load the Model and Class Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eaf5ae58-55c1-48bb-9dc3-40c3e483ff7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n_/vx3kcgzj67q_z0t840cm_5ym0000gn/T/ipykernel_71645/3066097055.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_data = torch.load(model_load_path, map_location=torch.device('cpu'))\n"
     ]
    }
   ],
   "source": [
    "# Load the model and class names\n",
    "model_load_path = '/Users/lennox/documents/machineLearning/research/builtModels/model_with_classes.pth'\n",
    "model_data = torch.load(model_load_path, map_location=torch.device('cpu'))\n",
    "\n",
    "# Retrieve the model state_dict and class names\n",
    "model_state_dict = model_data['model_state_dict']\n",
    "class_names = model_data['class_names']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6433e0e-e1a7-4b1d-be1e-13edcb515471",
   "metadata": {},
   "source": [
    "# Initialize the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a80e21b-aade-4195-b920-e50894bc8989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model (ResNet18)\n",
    "weights = models.ResNet18_Weights.DEFAULT\n",
    "model = models.resnet18(weights=weights)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, len(class_names))  # Adjust the final layer to match the number of classes\n",
    "\n",
    "# Load the saved state_dict\n",
    "model.load_state_dict(model_state_dict)\n",
    "model.eval()  # Set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d73855-6012-4286-a60f-3805ba0fb79b",
   "metadata": {},
   "source": [
    "# Define the Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f8f8811-128d-4da8-a4d5-11368145b0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be1546b-a19e-4841-b277-a27c0ed9e936",
   "metadata": {},
   "source": [
    "# Define the Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54177084-b08b-475c-8e32-27416690b288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the class of a given image\n",
    "def predict_image(image_path, model, classes):\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert('RGB')  # Ensure image is in RGB mode\n",
    "    image = transform(image).unsqueeze(0).to(device)  # Add batch dimension and send to device\n",
    "    output = model(image)\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "    return classes[predicted.item()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf0210-867c-4e65-acff-0ec1d9633501",
   "metadata": {},
   "source": [
    "# Select a Random Image from the Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0da20320-612a-4a0b-9efc-6bc9762b003d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random image from the folder\n",
    "image_folder_path = '/Users/lennox/Documents/machineLearning/data/tenAnimalsImages'\n",
    "random_image = random.choice(os.listdir(image_folder_path))\n",
    "image_path = os.path.join(image_folder_path, random_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b825a5f-d88d-4262-9e74-7adec9ac9513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get a list of all image files in the dataset directory\n",
    "def get_all_image_paths(dataset_path):\n",
    "    image_paths = []\n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                image_paths.append(os.path.join(root, file))\n",
    "    return image_paths\n",
    "\n",
    "# Get all image paths\n",
    "all_image_paths = get_all_image_paths(dataset_path)\n",
    "\n",
    "# Select a random image from the list\n",
    "random_image_path = random.choice(all_image_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64781aeb-b050-41d3-a72e-96df7da5c9b8",
   "metadata": {},
   "source": [
    "# Predict the Class of the Random Image and Print Actual and Predicted Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "304d9340-68d4-4d13-a2a5-80f6fb65f97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Path: /Users/lennox/Documents/machineLearning/data/tenAnimalsImages/chicken/503.jpeg\n",
      "Actual Class: chicken\n",
      "Predicted Class: horse\n"
     ]
    }
   ],
   "source": [
    "# Predict the class of the random image\n",
    "predicted_class = predict_image(random_image_path, model, class_names)\n",
    "\n",
    "# Extract the actual class from the image path\n",
    "actual_class = os.path.basename(os.path.dirname(random_image_path))\n",
    "\n",
    "print(f'Image Path: {random_image_path}')\n",
    "print(f'Actual Class: {actual_class}')\n",
    "print(f'Predicted Class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1285f476-bb67-45f6-88ba-8fffbb7fdc37",
   "metadata": {},
   "source": [
    "# Display the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4314f279-3861-48b9-8609-07607e15be6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image\n",
    "image = Image.open(random_image_path)\n",
    "image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3745bc7e-d79e-4c7e-9f28-38c3a21ef339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2f04f0-8759-4ce8-810a-a1f723d8d2c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a68e3137-2061-4d73-bcfb-e1543c8a5166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8f23ecb4-cf79-4ee0-828d-2620a1938832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, 10)  # Adjust to your number of classes\n",
    "\n",
    "# Freeze the layers except the final fully connected layer\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the final fully connected layer\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b8349a53-d47e-4c87-9db1-c70e73829ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Adjust learning rate and number of epochs\n",
    "optimizer = torch.optim.SGD(model.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "num_epochs = 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "33f1b4d8-9936-4e33-bd9f-7872b2a24aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet50(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, 10)  # Adjust to your number of classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "444d54b7-59b1-4aea-afac-2f51302c7bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(64 * 28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = nn.functional.relu(self.conv3(x))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = CustomCNN(num_classes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dc8234c1-74ed-4952-93f5-7dc17d5e0c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "88016351-f738-4bac-9ead-7d554789462b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Epoch 1/25, Loss: 0.5445, Accuracy: 0.8331\n",
      "Epoch 2/25, Loss: 0.3816, Accuracy: 0.8770\n",
      "Epoch 3/25, Loss: 0.3717, Accuracy: 0.8784\n",
      "Epoch 4/25, Loss: 0.3646, Accuracy: 0.8790\n",
      "Epoch 5/25, Loss: 0.3610, Accuracy: 0.8800\n",
      "Epoch 6/25, Loss: 0.3657, Accuracy: 0.8799\n",
      "Epoch 7/25, Loss: 0.3578, Accuracy: 0.8835\n",
      "Epoch 8/25, Loss: 0.3441, Accuracy: 0.8866\n",
      "Epoch 9/25, Loss: 0.3519, Accuracy: 0.8843\n",
      "Epoch 10/25, Loss: 0.3474, Accuracy: 0.8856\n",
      "Epoch 11/25, Loss: 0.3541, Accuracy: 0.8837\n",
      "Epoch 12/25, Loss: 0.3526, Accuracy: 0.8845\n",
      "Epoch 13/25, Loss: 0.3475, Accuracy: 0.8835\n",
      "Epoch 14/25, Loss: 0.3466, Accuracy: 0.8860\n",
      "Epoch 15/25, Loss: 0.3442, Accuracy: 0.8871\n",
      "Epoch 16/25, Loss: 0.3432, Accuracy: 0.8865\n",
      "Epoch 17/25, Loss: 0.3481, Accuracy: 0.8854\n",
      "Epoch 18/25, Loss: 0.3428, Accuracy: 0.8852\n",
      "Epoch 19/25, Loss: 0.3407, Accuracy: 0.8869\n",
      "Epoch 20/25, Loss: 0.3452, Accuracy: 0.8869\n",
      "Epoch 21/25, Loss: 0.3398, Accuracy: 0.8865\n",
      "Epoch 22/25, Loss: 0.3452, Accuracy: 0.8840\n",
      "Epoch 23/25, Loss: 0.3378, Accuracy: 0.8904\n",
      "Epoch 24/25, Loss: 0.3422, Accuracy: 0.8883\n",
      "Epoch 25/25, Loss: 0.3430, Accuracy: 0.8879\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = datasets.ImageFolder(root='/Users/lennox/Documents/machineLearning/data/tenAnimalsImages', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Load pre-trained model and modify\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, 10)  # Adjust to your number of classes\n",
    "\n",
    "# Freeze the layers except the final fully connected layer\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Move model to device\n",
    "# Check if MPS is available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 25\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = correct / total\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a0630a-0885-4ca0-beb2-014e6580a0c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
